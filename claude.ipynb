{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33bd108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: pyinform not available. Using alternative mutual information calculation.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Time series analysis\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.stattools import acf, pacf, adfuller, kpss\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from arch import arch_model\n",
    "from pygam import LinearGAM, s\n",
    "\n",
    "# Statistical tests\n",
    "from scipy import stats\n",
    "from scipy.signal import periodogram, find_peaks\n",
    "from scipy.stats import ks_2samp, anderson, jarque_bera, entropy, wasserstein_distance\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Advanced time series\n",
    "try:\n",
    "    import nolds  # for Hurst exponent\n",
    "except ImportError:\n",
    "    print(\"Warning: nolds not available. Some non-linear metrics will be skipped.\")\n",
    "    nolds = None\n",
    "\n",
    "try:\n",
    "    from pyinform import mutual_info, transfer_entropy\n",
    "    PYINFORM_AVAILABLE = True\n",
    "except (ImportError, OSError):\n",
    "    print(\"Warning: pyinform not available. Using alternative mutual information calculation.\")\n",
    "    PYINFORM_AVAILABLE = False\n",
    "\n",
    "# Visualization\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0fe3d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: 52696 observations\n",
      "Training: 42156, Testing: 10540\n",
      "Temperature range: [-6.4, 34.8] °C\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Enhanced Data Loading and Preprocessing\n",
    "class TimeSeriesDataHandler:\n",
    "    def __init__(self, data_path=None, synthetic_data=None):\n",
    "        if data_path:\n",
    "            try:\n",
    "                # Try to read the CSV file\n",
    "                self.df = pd.read_csv(data_path)\n",
    "                \n",
    "                # Handle different possible column names for temperature\n",
    "                temp_columns = ['T (degC)', 'temperature', 'temp', 'Temperature', 'TEMP']\n",
    "                date_columns = ['date', 'Date', 'DATE', 'time', 'Time', 'TIME', 'datetime']\n",
    "                \n",
    "                # Find temperature column\n",
    "                temp_col = None\n",
    "                for col in temp_columns:\n",
    "                    if col in self.df.columns:\n",
    "                        temp_col = col\n",
    "                        break\n",
    "                \n",
    "                if temp_col is None:\n",
    "                    print(f\"Available columns: {list(self.df.columns)}\")\n",
    "                    # Use the first numeric column\n",
    "                    numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "                    if len(numeric_cols) > 0:\n",
    "                        temp_col = numeric_cols[0]\n",
    "                        print(f\"Using column '{temp_col}' as temperature data\")\n",
    "                    else:\n",
    "                        raise ValueError(\"No numeric columns found in the CSV file\")\n",
    "                \n",
    "                # Try to parse dates if available\n",
    "                date_col = None\n",
    "                for col in date_columns:\n",
    "                    if col in self.df.columns:\n",
    "                        date_col = col\n",
    "                        break\n",
    "                \n",
    "                if date_col:\n",
    "                    try:\n",
    "                        self.df[date_col] = pd.to_datetime(self.df[date_col])\n",
    "                        self.df = self.df.set_index(date_col)\n",
    "                    except:\n",
    "                        print(f\"Could not parse dates from column '{date_col}'\")\n",
    "                \n",
    "                self.y = self.df[temp_col].values\n",
    "                \n",
    "                # Remove any NaN values\n",
    "                self.y = self.y[~np.isnan(self.y)]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading data from {data_path}: {e}\")\n",
    "                print(\"Generating synthetic example data instead...\")\n",
    "                self.y = self._generate_example_data()\n",
    "                \n",
    "        elif synthetic_data is not None:\n",
    "            self.y = synthetic_data\n",
    "        else:\n",
    "            # Generate example data if no path provided\n",
    "            self.y = self._generate_example_data()\n",
    "        \n",
    "        self.n = len(self.y)\n",
    "        self.train_end = int(0.8 * self.n)\n",
    "        self.y_train = self.y[:self.train_end]\n",
    "        self.y_test = self.y[self.train_end:]\n",
    "        self.time = np.arange(self.n)\n",
    "        \n",
    "    def _generate_example_data(self, n=2000):\n",
    "        \"\"\"Generate example temperature-like data\"\"\"\n",
    "        t = np.arange(n)\n",
    "        trend = 15 + 0.001 * t  # Slow warming trend\n",
    "        season = 10 * np.sin(2 * np.pi * t / 365.25)  # Annual cycle\n",
    "        daily = 3 * np.sin(2 * np.pi * t / 1)  # Daily cycle\n",
    "        noise = np.random.normal(0, 2, n)\n",
    "        \n",
    "        # Add some non-linear dynamics\n",
    "        ar_noise = np.zeros(n)\n",
    "        for i in range(1, n):\n",
    "            ar_noise[i] = 0.7 * ar_noise[i-1] + noise[i]\n",
    "        \n",
    "        return trend + season + daily + ar_noise\n",
    "    \n",
    "    def get_train_test_split(self):\n",
    "        return self.y_train, self.y_test, self.time[:self.train_end], self.time[self.train_end:]\n",
    "\n",
    "# Initialize data handler with real data path\n",
    "data_path = './data/weather/temp.csv'\n",
    "data_handler = TimeSeriesDataHandler(data_path=data_path)\n",
    "y_train, y_test, time_train, time_test = data_handler.get_train_test_split()\n",
    "\n",
    "print(f\"Data shape: {data_handler.n} observations\")\n",
    "print(f\"Training: {len(y_train)}, Testing: {len(y_test)}\")\n",
    "print(f\"Temperature range: [{y_train.min():.1f}, {y_train.max():.1f}] °C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting synthetic data generator...\n",
      "  - STL decomposition...\n",
      "    Using period=365\n",
      "  - Trend model...\n",
      "  - Multi-scale seasonality...\n",
      "  - GARCH model for residuals...\n",
      "  - Extreme value model...\n",
      "  - Regime detection...\n",
      "Generator fitting complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Enhanced Synthetic Data Generator\n",
    "class AdvancedSyntheticGenerator:\n",
    "    def __init__(self, y_train, time_train):\n",
    "        self.y_train = y_train\n",
    "        self.time_train = time_train\n",
    "        self.n_train = len(y_train)\n",
    "        self.models = {}\n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"Fit all component models\"\"\"\n",
    "        print(\"Fitting synthetic data generator...\")\n",
    "        \n",
    "        # 1. STL Decomposition\n",
    "        print(\"  - STL decomposition...\")\n",
    "        # Try multiple periods to find the best one\n",
    "        periods_to_try = [365, 144, 24, 12, 7]\n",
    "        best_period = None\n",
    "        \n",
    "        for period in periods_to_try:\n",
    "            if period < len(self.y_train) / 2:\n",
    "                try:\n",
    "                    self.stl = STL(self.y_train, period=period, robust=True)\n",
    "                    self.stl_result = self.stl.fit()\n",
    "                    best_period = period\n",
    "                    print(f\"    Using period={period}\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        if best_period is None:\n",
    "            # Fallback to simple decomposition\n",
    "            print(\"    STL failed, using simple decomposition\")\n",
    "            window = min(365, len(self.y_train) // 4)\n",
    "            self.stl_result = self._simple_decomposition(window)\n",
    "        \n",
    "        # 2. Trend modeling with polynomial or spline\n",
    "        print(\"  - Trend model...\")\n",
    "        self._fit_trend()\n",
    "        \n",
    "        # 3. Multiple seasonality with Fourier terms\n",
    "        print(\"  - Multi-scale seasonality...\")\n",
    "        self._fit_seasonality()\n",
    "        \n",
    "        # 4. Residual modeling - GARCH + Jump diffusion\n",
    "        print(\"  - GARCH model for residuals...\")\n",
    "        residuals = self.stl_result.resid\n",
    "        \n",
    "        # Remove NaN values for GARCH fitting\n",
    "        clean_residuals = residuals[~np.isnan(residuals)]\n",
    "        self.residual_std = np.std(clean_residuals)  # Always store this\n",
    "        \n",
    "        try:\n",
    "            self.garch = arch_model(clean_residuals, vol='Garch', p=1, q=1, dist='normal')\n",
    "            self.garch_result = self.garch.fit(disp='off')\n",
    "        except:\n",
    "            print(\"    GARCH fitting failed, using simple volatility model\")\n",
    "            self.garch_result = None\n",
    "        \n",
    "        # 5. Extreme value model\n",
    "        print(\"  - Extreme value model...\")\n",
    "        self.threshold = np.percentile(np.abs(clean_residuals), 95)\n",
    "        self.extremes = clean_residuals[np.abs(clean_residuals) > self.threshold]\n",
    "        self.extreme_rate = len(self.extremes) / len(clean_residuals)\n",
    "        \n",
    "        # 6. Regime detection\n",
    "        print(\"  - Regime detection...\")\n",
    "        self._detect_regimes()\n",
    "        \n",
    "        print(\"Generator fitting complete!\")\n",
    "    \n",
    "    def _simple_decomposition(self, window):\n",
    "        \"\"\"Simple decomposition when STL fails\"\"\"\n",
    "        from collections import namedtuple\n",
    "        \n",
    "        # Trend: moving average\n",
    "        trend = pd.Series(self.y_train).rolling(window=window, center=True).mean().values\n",
    "        \n",
    "        # Detrend\n",
    "        detrended = self.y_train - trend\n",
    "        detrended_clean = detrended[~np.isnan(detrended)]\n",
    "        \n",
    "        # Seasonal: average by position in cycle\n",
    "        if len(detrended_clean) > window:\n",
    "            seasonal = np.zeros_like(self.y_train)\n",
    "            for i in range(window):\n",
    "                indices = np.arange(i, len(self.y_train), window)\n",
    "                if len(indices) > 0:\n",
    "                    seasonal[indices] = np.nanmean(detrended[indices])\n",
    "        else:\n",
    "            seasonal = np.zeros_like(self.y_train)\n",
    "        \n",
    "        # Residual\n",
    "        resid = self.y_train - trend - seasonal\n",
    "        \n",
    "        STLResult = namedtuple('STLResult', ['trend', 'seasonal', 'resid'])\n",
    "        return STLResult(trend=trend, seasonal=seasonal, resid=resid)\n",
    "    \n",
    "    def _fit_trend(self):\n",
    "        \"\"\"Fit trend using polynomial regression\"\"\"\n",
    "        # Remove NaN values\n",
    "        mask = ~np.isnan(self.stl_result.trend)\n",
    "        x = self.time_train[mask]\n",
    "        y = self.stl_result.trend[mask]\n",
    "        \n",
    "        # Fit polynomial (degree 3 for flexibility)\n",
    "        self.trend_poly = np.polyfit(x, y, deg=3)\n",
    "        \n",
    "    def _fit_seasonality(self):\n",
    "        \"\"\"Fit multiple seasonality components\"\"\"\n",
    "        # Potential seasonal periods (in descending order)\n",
    "        self.seasonal_periods = []\n",
    "        \n",
    "        # Detect periods using periodogram\n",
    "        from scipy.signal import periodogram\n",
    "        frequencies, power = periodogram(self.stl_result.seasonal[~np.isnan(self.stl_result.seasonal)])\n",
    "        \n",
    "        # Find peaks in power spectrum\n",
    "        peaks, _ = find_peaks(power, height=np.percentile(power, 90))\n",
    "        \n",
    "        if len(peaks) > 0:\n",
    "            # Convert peak frequencies to periods\n",
    "            for peak in peaks[:3]:  # Top 3 peaks\n",
    "                if frequencies[peak] > 0:\n",
    "                    period = int(1 / frequencies[peak])\n",
    "                    if 2 <= period <= len(self.y_train) // 2:\n",
    "                        self.seasonal_periods.append(period)\n",
    "        \n",
    "        # Default periods if none detected\n",
    "        if len(self.seasonal_periods) == 0:\n",
    "            self.seasonal_periods = [365, 7, 1]\n",
    "        \n",
    "        # Fit Fourier series for each period\n",
    "        self.seasonal_models = {}\n",
    "        seasonal_clean = self.stl_result.seasonal[~np.isnan(self.stl_result.seasonal)]\n",
    "        time_clean = self.time_train[~np.isnan(self.stl_result.seasonal)]\n",
    "        \n",
    "        for period in self.seasonal_periods:\n",
    "            n_harmonics = min(3, period // 2)\n",
    "            fourier_features = []\n",
    "            \n",
    "            for k in range(1, n_harmonics + 1):\n",
    "                fourier_features.append(np.sin(2 * np.pi * k * time_clean / period))\n",
    "                fourier_features.append(np.cos(2 * np.pi * k * time_clean / period))\n",
    "            \n",
    "            X_fourier = np.column_stack(fourier_features)\n",
    "            \n",
    "            # Fit using Ridge regression\n",
    "            model = Ridge(alpha=0.1)\n",
    "            model.fit(X_fourier, seasonal_clean)\n",
    "            self.seasonal_models[period] = (model, n_harmonics)\n",
    "        \n",
    "    def _detect_regimes(self):\n",
    "        \"\"\"Detect regime changes in volatility\"\"\"\n",
    "        residuals = self.stl_result.resid[~np.isnan(self.stl_result.resid)]\n",
    "        rolling_std = pd.Series(residuals).rolling(30).std()\n",
    "        self.volatility_regimes = (rolling_std > rolling_std.median()).astype(int)\n",
    "        \n",
    "    def generate(self, n_samples, include_extremes=True):\n",
    "        \"\"\"Generate synthetic time series\"\"\"\n",
    "        time_full = np.arange(n_samples)\n",
    "        \n",
    "        # 1. Generate trend\n",
    "        trend = np.polyval(self.trend_poly, time_full)\n",
    "        \n",
    "        # 2. Generate multi-scale seasonality\n",
    "        seasonal = np.zeros(n_samples)\n",
    "        \n",
    "        for period, (model, n_harmonics) in self.seasonal_models.items():\n",
    "            fourier_features = []\n",
    "            \n",
    "            for k in range(1, n_harmonics + 1):\n",
    "                fourier_features.append(np.sin(2 * np.pi * k * time_full / period))\n",
    "                fourier_features.append(np.cos(2 * np.pi * k * time_full / period))\n",
    "            \n",
    "            X_fourier = np.column_stack(fourier_features)\n",
    "            seasonal += model.predict(X_fourier) / len(self.seasonal_periods)\n",
    "        \n",
    "        # 3. Generate residuals\n",
    "        if self.garch_result is not None:\n",
    "            # GARCH residuals\n",
    "            try:\n",
    "                # Use the forecast method to simulate\n",
    "                forecasts = self.garch_result.forecast(horizon=n_samples, method='simulation')\n",
    "                residuals = np.zeros(n_samples)\n",
    "                \n",
    "                # Generate using the conditional variance\n",
    "                cond_var = self.garch_result.conditional_volatility[-1]\n",
    "                \n",
    "                for i in range(n_samples):\n",
    "                    # Simple GARCH(1,1) simulation\n",
    "                    if i == 0:\n",
    "                        residuals[i] = np.random.normal(0, cond_var)\n",
    "                    else:\n",
    "                        # Update conditional variance\n",
    "                        omega = self.garch_result.params['omega']\n",
    "                        alpha = self.garch_result.params['alpha[1]']\n",
    "                        beta = self.garch_result.params['beta[1]']\n",
    "                        \n",
    "                        cond_var = omega + alpha * residuals[i-1]**2 + beta * cond_var**2\n",
    "                        cond_var = np.sqrt(max(cond_var, 1e-6))  # Ensure positive\n",
    "                        residuals[i] = np.random.normal(0, cond_var)\n",
    "                        \n",
    "            except:\n",
    "                # Fallback to simple volatility clustering\n",
    "                residuals = np.random.normal(0, self.residual_std, n_samples)\n",
    "                # Add some volatility clustering\n",
    "                for i in range(1, n_samples):\n",
    "                    if np.abs(residuals[i-1]) > 2 * self.residual_std:\n",
    "                        residuals[i] *= 1.5  # Higher volatility after extreme values\n",
    "        else:\n",
    "            # Simple random residuals\n",
    "            residuals = np.random.normal(0, self.residual_std, n_samples)\n",
    "        \n",
    "        # 4. Add extreme events\n",
    "        if include_extremes and len(self.extremes) > 0:\n",
    "            n_extremes = int(n_samples * self.extreme_rate)\n",
    "            if n_extremes > 0:\n",
    "                extreme_indices = np.random.choice(n_samples, n_extremes, replace=False)\n",
    "                extreme_values = np.random.choice(self.extremes, n_extremes, replace=True)\n",
    "                residuals[extreme_indices] = extreme_values\n",
    "        \n",
    "        # 5. Combine components\n",
    "        synthetic = trend + seasonal + residuals\n",
    "        \n",
    "        return synthetic\n",
    "    \n",
    "    def get_components(self):\n",
    "        \"\"\"Return fitted components for visualization\"\"\"\n",
    "        return {\n",
    "            'trend': self.stl_result.trend,\n",
    "            'seasonal': self.stl_result.seasonal,\n",
    "            'residual': self.stl_result.resid\n",
    "        }\n",
    "\n",
    "# Fit the generator\n",
    "generator = AdvancedSyntheticGenerator(y_train, time_train)\n",
    "generator.fit()\n",
    "\n",
    "# Generate synthetic data\n",
    "y_synth_full = generator.generate(len(data_handler.y))\n",
    "y_synth_test = y_synth_full[data_handler.train_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7afe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Comprehensive Evaluation Framework\n",
    "class ComprehensiveEvaluator:\n",
    "    def __init__(self, y_real, y_synth, y_real_train=None):\n",
    "        self.y_real = y_real\n",
    "        self.y_synth = y_synth\n",
    "        self.y_real_train = y_real_train\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate_all(self):\n",
    "        \"\"\"Run all evaluation metrics\"\"\"\n",
    "        print(\"Running comprehensive evaluation...\")\n",
    "        \n",
    "        # 1. Distributional tests\n",
    "        self._evaluate_distributions()\n",
    "        \n",
    "        # 2. Temporal structure tests\n",
    "        self._evaluate_temporal_structure()\n",
    "        \n",
    "        # 3. Spectral analysis\n",
    "        self._evaluate_spectral_properties()\n",
    "        \n",
    "        # 4. Extreme value analysis\n",
    "        self._evaluate_extremes()\n",
    "        \n",
    "        # 5. Non-linear dynamics\n",
    "        self._evaluate_nonlinear_dynamics()\n",
    "        \n",
    "        # 6. Predictive utility\n",
    "        self._evaluate_predictive_utility()\n",
    "        \n",
    "        # 7. Domain-specific tests\n",
    "        self._evaluate_domain_constraints()\n",
    "        \n",
    "        # 8. Machine learning discriminability\n",
    "        self._evaluate_ml_discriminability()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def _evaluate_distributions(self):\n",
    "        \"\"\"Test marginal and joint distributions\"\"\"\n",
    "        print(\"  1. Distributional tests...\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        self.results['mean_diff'] = np.abs(np.mean(self.y_real) - np.mean(self.y_synth))\n",
    "        self.results['std_diff'] = np.abs(np.std(self.y_real) - np.std(self.y_synth))\n",
    "        self.results['skew_diff'] = np.abs(stats.skew(self.y_real) - stats.skew(self.y_synth))\n",
    "        self.results['kurt_diff'] = np.abs(stats.kurtosis(self.y_real) - stats.kurtosis(self.y_synth))\n",
    "        \n",
    "        # Statistical tests\n",
    "        ks_stat, ks_pval = ks_2samp(self.y_real, self.y_synth)\n",
    "        self.results['ks_statistic'] = ks_stat\n",
    "        self.results['ks_pvalue'] = ks_pval\n",
    "        \n",
    "        # Anderson-Darling test\n",
    "        ad_result = stats.anderson_ksamp([self.y_real, self.y_synth])\n",
    "        self.results['anderson_statistic'] = ad_result.statistic\n",
    "        self.results['anderson_pvalue'] = ad_result.significance_level\n",
    "        \n",
    "        # Wasserstein distance\n",
    "        self.results['wasserstein_distance'] = wasserstein_distance(self.y_real, self.y_synth)\n",
    "        \n",
    "        # Jarque-Bera normality test\n",
    "        jb_real = jarque_bera(self.y_real)\n",
    "        jb_synth = jarque_bera(self.y_synth)\n",
    "        self.results['jb_real'] = jb_real.statistic\n",
    "        self.results['jb_synth'] = jb_synth.statistic\n",
    "        \n",
    "    def _evaluate_temporal_structure(self):\n",
    "        \"\"\"Test temporal dependencies\"\"\"\n",
    "        print(\"  2. Temporal structure tests...\")\n",
    "        \n",
    "        # ACF comparison\n",
    "        max_lag = min(50, len(self.y_real) // 4)\n",
    "        acf_real = acf(self.y_real, nlags=max_lag, fft=True)\n",
    "        acf_synth = acf(self.y_synth, nlags=max_lag, fft=True)\n",
    "        \n",
    "        # Weighted ACF difference\n",
    "        weights = 1 / (np.arange(max_lag + 1) + 1)\n",
    "        self.results['weighted_acf_l2'] = np.sum(weights * (acf_real - acf_synth)**2)\n",
    "        \n",
    "        # Ljung-Box test for autocorrelation\n",
    "        lb_real = acorr_ljungbox(self.y_real, lags=20, return_df=True)\n",
    "        lb_synth = acorr_ljungbox(self.y_synth, lags=20, return_df=True)\n",
    "        self.results['ljungbox_real_pval'] = lb_real['lb_pvalue'].min()\n",
    "        self.results['ljungbox_synth_pval'] = lb_synth['lb_pvalue'].min()\n",
    "        \n",
    "        # Stationarity tests\n",
    "        adf_real = adfuller(self.y_real)\n",
    "        adf_synth = adfuller(self.y_synth)\n",
    "        self.results['adf_real_pval'] = adf_real[1]\n",
    "        self.results['adf_synth_pval'] = adf_synth[1]\n",
    "        \n",
    "    def _evaluate_spectral_properties(self):\n",
    "        \"\"\"Test frequency domain properties\"\"\"\n",
    "        print(\"  3. Spectral analysis...\")\n",
    "        \n",
    "        # Power spectral density\n",
    "        freq_real, psd_real = periodogram(self.y_real)\n",
    "        freq_synth, psd_synth = periodogram(self.y_synth)\n",
    "        \n",
    "        # Normalize PSDs\n",
    "        psd_real_norm = psd_real / psd_real.sum()\n",
    "        psd_synth_norm = psd_synth / psd_synth.sum()\n",
    "        \n",
    "        # KL divergence in frequency domain\n",
    "        self.results['spectral_kl'] = entropy(psd_real_norm, psd_synth_norm)\n",
    "        \n",
    "        # Peak frequency comparison\n",
    "        peaks_real, _ = find_peaks(psd_real, height=np.percentile(psd_real, 90))\n",
    "        peaks_synth, _ = find_peaks(psd_synth, height=np.percentile(psd_synth, 90))\n",
    "        \n",
    "        if len(peaks_real) > 0 and len(peaks_synth) > 0:\n",
    "            self.results['peak_freq_diff'] = np.abs(freq_real[peaks_real[0]] - freq_synth[peaks_synth[0]])\n",
    "        else:\n",
    "            self.results['peak_freq_diff'] = np.nan\n",
    "            \n",
    "    def _evaluate_extremes(self):\n",
    "        \"\"\"Test extreme value properties\"\"\"\n",
    "        print(\"  4. Extreme value analysis...\")\n",
    "        \n",
    "        # Define thresholds\n",
    "        thresholds = [90, 95, 99]\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            # Upper tail\n",
    "            upper_thresh_real = np.percentile(self.y_real, thresh)\n",
    "            upper_thresh_synth = np.percentile(self.y_synth, thresh)\n",
    "            \n",
    "            exceed_rate_real = np.mean(self.y_real > upper_thresh_real)\n",
    "            exceed_rate_synth = np.mean(self.y_synth > upper_thresh_synth)\n",
    "            \n",
    "            self.results[f'exceed_rate_{thresh}_diff'] = np.abs(exceed_rate_real - exceed_rate_synth)\n",
    "            \n",
    "            # Lower tail\n",
    "            lower_thresh_real = np.percentile(self.y_real, 100 - thresh)\n",
    "            lower_rate_real = np.mean(self.y_real < lower_thresh_real)\n",
    "            lower_rate_synth = np.mean(self.y_synth < np.percentile(self.y_synth, 100 - thresh))\n",
    "            \n",
    "            self.results[f'lower_exceed_rate_{thresh}_diff'] = np.abs(lower_rate_real - lower_rate_synth)\n",
    "        \n",
    "        # Extreme value clustering\n",
    "        extreme_indices_real = np.where(np.abs(self.y_real - np.mean(self.y_real)) > 2 * np.std(self.y_real))[0]\n",
    "        extreme_indices_synth = np.where(np.abs(self.y_synth - np.mean(self.y_synth)) > 2 * np.std(self.y_synth))[0]\n",
    "        \n",
    "        if len(extreme_indices_real) > 1 and len(extreme_indices_synth) > 1:\n",
    "            cluster_real = np.mean(np.diff(extreme_indices_real))\n",
    "            cluster_synth = np.mean(np.diff(extreme_indices_synth))\n",
    "            self.results['extreme_clustering_diff'] = np.abs(cluster_real - cluster_synth)\n",
    "        else:\n",
    "            self.results['extreme_clustering_diff'] = np.nan\n",
    "            \n",
    "    def _evaluate_nonlinear_dynamics(self):\n",
    "        \"\"\"Test non-linear properties\"\"\"\n",
    "        print(\"  5. Non-linear dynamics...\")\n",
    "        \n",
    "        # Hurst exponent (long-range dependence)\n",
    "        if nolds is not None:\n",
    "            try:\n",
    "                H_real = nolds.hurst_rs(self.y_real)\n",
    "                H_synth = nolds.hurst_rs(self.y_synth)\n",
    "                self.results['hurst_diff'] = np.abs(H_real - H_synth)\n",
    "            except:\n",
    "                self.results['hurst_diff'] = np.nan\n",
    "        else:\n",
    "            self.results['hurst_diff'] = np.nan\n",
    "        \n",
    "        # Sample entropy (complexity)\n",
    "        if nolds is not None:\n",
    "            try:\n",
    "                samp_ent_real = nolds.sampen(self.y_real[:500])  # Use subset for speed\n",
    "                samp_ent_synth = nolds.sampen(self.y_synth[:500])\n",
    "                self.results['sample_entropy_diff'] = np.abs(samp_ent_real - samp_ent_synth)\n",
    "            except:\n",
    "                self.results['sample_entropy_diff'] = np.nan\n",
    "        else:\n",
    "            self.results['sample_entropy_diff'] = np.nan\n",
    "        \n",
    "        # Mutual information (non-linear correlation)\n",
    "        lag = 1\n",
    "        if PYINFORM_AVAILABLE:\n",
    "            mi_real = mutual_info.mutual_info(self.y_real[:-lag], self.y_real[lag:])\n",
    "            mi_synth = mutual_info.mutual_info(self.y_synth[:-lag], self.y_synth[lag:])\n",
    "        else:\n",
    "            # Alternative mutual information calculation using sklearn\n",
    "            from sklearn.feature_selection import mutual_info_regression\n",
    "            \n",
    "            # Discretize for mutual information\n",
    "            n_bins = 10\n",
    "            real_disc = pd.cut(self.y_real[:-lag], bins=n_bins, labels=False)\n",
    "            real_disc_lag = pd.cut(self.y_real[lag:], bins=n_bins, labels=False)\n",
    "            synth_disc = pd.cut(self.y_synth[:-lag], bins=n_bins, labels=False)\n",
    "            synth_disc_lag = pd.cut(self.y_synth[lag:], bins=n_bins, labels=False)\n",
    "            \n",
    "            # Calculate mutual information\n",
    "            mi_real = mutual_info_regression(real_disc.reshape(-1, 1), real_disc_lag, discrete_features=True)[0]\n",
    "            mi_synth = mutual_info_regression(synth_disc.reshape(-1, 1), synth_disc_lag, discrete_features=True)[0]\n",
    "            \n",
    "        self.results['mutual_info_diff'] = np.abs(mi_real - mi_synth)\n",
    "        \n",
    "    def _evaluate_predictive_utility(self):\n",
    "        \"\"\"Test predictive performance\"\"\"\n",
    "        print(\"  6. Predictive utility tests...\")\n",
    "        \n",
    "        # Prepare data for time series prediction\n",
    "        window = 24  # Look-back window\n",
    "        \n",
    "        def prepare_ts_data(data):\n",
    "            X, y = [], []\n",
    "            for i in range(window, len(data)):\n",
    "                X.append(data[i-window:i])\n",
    "                y.append(data[i])\n",
    "            return np.array(X), np.array(y)\n",
    "        \n",
    "        # TSTR: Train on Synthetic, Test on Real\n",
    "        X_synth, y_synth = prepare_ts_data(self.y_synth)\n",
    "        X_real, y_real = prepare_ts_data(self.y_real)\n",
    "        \n",
    "        # Simple models\n",
    "        models = {\n",
    "            'ridge': Ridge(alpha=1.0),\n",
    "            'rf': RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
    "        }\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            if name == 'ridge':\n",
    "                # Regression task\n",
    "                model.fit(X_synth[:len(X_synth)//2], y_synth[:len(y_synth)//2])\n",
    "                pred = model.predict(X_real[len(X_real)//2:])\n",
    "                self.results[f'tstr_{name}_mse'] = mean_squared_error(y_real[len(y_real)//2:], pred)\n",
    "            else:\n",
    "                # Classification task (above/below median)\n",
    "                y_synth_class = (y_synth > np.median(y_synth)).astype(int)\n",
    "                y_real_class = (y_real > np.median(y_real)).astype(int)\n",
    "                \n",
    "                model.fit(X_synth[:len(X_synth)//2], y_synth_class[:len(y_synth_class)//2])\n",
    "                pred_proba = model.predict_proba(X_real[len(X_real)//2:])[:, 1]\n",
    "                self.results[f'tstr_{name}_auc'] = roc_auc_score(y_real_class[len(y_real_class)//2:], pred_proba)\n",
    "        \n",
    "    def _evaluate_domain_constraints(self):\n",
    "        \"\"\"Test domain-specific properties\"\"\"\n",
    "        print(\"  7. Domain constraint tests...\")\n",
    "        \n",
    "        # Physical plausibility (temperature)\n",
    "        self.results['min_temp_real'] = self.y_real.min()\n",
    "        self.results['min_temp_synth'] = self.y_synth.min()\n",
    "        self.results['max_temp_real'] = self.y_real.max()\n",
    "        self.results['max_temp_synth'] = self.y_synth.max()\n",
    "        \n",
    "        # Check for unrealistic values\n",
    "        self.results['synth_below_absolute_zero'] = np.sum(self.y_synth < -273.15)\n",
    "        self.results['synth_above_boiling'] = np.sum(self.y_synth > 100)\n",
    "        \n",
    "        # Gradient constraints (rate of change)\n",
    "        grad_real = np.abs(np.diff(self.y_real))\n",
    "        grad_synth = np.abs(np.diff(self.y_synth))\n",
    "        \n",
    "        self.results['max_gradient_real'] = grad_real.max()\n",
    "        self.results['max_gradient_synth'] = grad_synth.max()\n",
    "        self.results['gradient_percentile_95_diff'] = np.abs(\n",
    "            np.percentile(grad_real, 95) - np.percentile(grad_synth, 95)\n",
    "        )\n",
    "        \n",
    "    def _evaluate_ml_discriminability(self):\n",
    "        \"\"\"Test if ML can distinguish real from synthetic\"\"\"\n",
    "        print(\"  8. ML discriminability tests...\")\n",
    "        \n",
    "        # Create windows\n",
    "        window_size = 50\n",
    "        stride = 25\n",
    "        \n",
    "        def create_windows(data, label):\n",
    "            windows = []\n",
    "            labels = []\n",
    "            for i in range(0, len(data) - window_size, stride):\n",
    "                windows.append(data[i:i+window_size])\n",
    "                labels.append(label)\n",
    "            return windows, labels\n",
    "        \n",
    "        # Create dataset\n",
    "        real_windows, real_labels = create_windows(self.y_real, 0)\n",
    "        synth_windows, synth_labels = create_windows(self.y_synth, 1)\n",
    "        \n",
    "        X = np.vstack([real_windows, synth_windows])\n",
    "        y = np.hstack([real_labels, synth_labels])\n",
    "        \n",
    "        # Extract features\n",
    "        features = []\n",
    "        for window in X:\n",
    "            feat = [\n",
    "                np.mean(window),\n",
    "                np.std(window),\n",
    "                stats.skew(window),\n",
    "                stats.kurtosis(window),\n",
    "                np.percentile(window, 25),\n",
    "                np.percentile(window, 75),\n",
    "                len(find_peaks(window)[0])\n",
    "            ]\n",
    "            features.append(feat)\n",
    "        \n",
    "        X_features = np.array(features)\n",
    "        \n",
    "        # Train classifier\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_features, y, test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "        self.results['discriminator_auc'] = roc_auc_score(y_test, pred_proba)\n",
    "        \n",
    "        # Feature importance\n",
    "        importances = rf.feature_importances_\n",
    "        feature_names = ['mean', 'std', 'skew', 'kurt', 'q25', 'q75', 'n_peaks']\n",
    "        self.results['top_discriminative_features'] = sorted(\n",
    "            zip(feature_names, importances), key=lambda x: x[1], reverse=True\n",
    "        )[:3]\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = ComprehensiveEvaluator(y_test, y_synth_test, y_train)\n",
    "results = evaluator.evaluate_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9fbfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Bootstrap Confidence Intervals\n",
    "class BootstrapEvaluator:\n",
    "    def __init__(self, generator, data_handler, n_bootstrap=100):\n",
    "        self.generator = generator\n",
    "        self.data_handler = data_handler\n",
    "        self.n_bootstrap = n_bootstrap\n",
    "        \n",
    "    def bootstrap_metrics(self, metric_func, **kwargs):\n",
    "        \"\"\"Bootstrap confidence intervals for any metric\"\"\"\n",
    "        values = []\n",
    "        \n",
    "        for i in range(self.n_bootstrap):\n",
    "            # Generate new synthetic data\n",
    "            y_synth = self.generator.generate(len(self.data_handler.y))\n",
    "            y_synth_test = y_synth[self.data_handler.train_end:]\n",
    "            \n",
    "            # Resample real data\n",
    "            indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "            y_real_boot = y_test[indices]\n",
    "            \n",
    "            # Calculate metric\n",
    "            value = metric_func(y_real_boot, y_synth_test, **kwargs)\n",
    "            values.append(value)\n",
    "        \n",
    "        values = np.array(values)\n",
    "        return {\n",
    "            'mean': np.mean(values),\n",
    "            'std': np.std(values),\n",
    "            'ci_lower': np.percentile(values, 2.5),\n",
    "            'ci_upper': np.percentile(values, 97.5)\n",
    "        }\n",
    "    \n",
    "    def run_bootstrap_evaluation(self):\n",
    "        \"\"\"Run bootstrap for key metrics\"\"\"\n",
    "        print(\"Running bootstrap evaluation (this may take a while)...\")\n",
    "        \n",
    "        # Define metrics\n",
    "        metrics = {\n",
    "            'ks_statistic': lambda r, s: ks_2samp(r, s)[0],\n",
    "            'wasserstein': lambda r, s: wasserstein_distance(r, s),\n",
    "            'mean_diff': lambda r, s: np.abs(np.mean(r) - np.mean(s)),\n",
    "            'std_diff': lambda r, s: np.abs(np.std(r) - np.std(s))\n",
    "        }\n",
    "        \n",
    "        bootstrap_results = {}\n",
    "        for name, func in metrics.items():\n",
    "            print(f\"  Bootstrapping {name}...\")\n",
    "            bootstrap_results[name] = self.bootstrap_metrics(func)\n",
    "        \n",
    "        return bootstrap_results\n",
    "\n",
    "# Run bootstrap evaluation\n",
    "bootstrap_eval = BootstrapEvaluator(generator, data_handler, n_bootstrap=50)\n",
    "bootstrap_results = bootstrap_eval.run_bootstrap_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8acaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualization Suite\n",
    "class ComprehensiveVisualizer:\n",
    "    def __init__(self, y_real, y_synth, results, bootstrap_results=None):\n",
    "        self.y_real = y_real\n",
    "        self.y_synth = y_synth\n",
    "        self.results = results\n",
    "        self.bootstrap_results = bootstrap_results\n",
    "        \n",
    "    def plot_all(self):\n",
    "        \"\"\"Create comprehensive visualization\"\"\"\n",
    "        fig = plt.figure(figsize=(20, 24))\n",
    "        gs = GridSpec(8, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # 1. Time series comparison\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        self._plot_time_series(ax1)\n",
    "        \n",
    "        # 2. Distribution comparison\n",
    "        ax2 = fig.add_subplot(gs[1, 0])\n",
    "        self._plot_distributions(ax2)\n",
    "        \n",
    "        # 3. Q-Q plot\n",
    "        ax3 = fig.add_subplot(gs[1, 1])\n",
    "        self._plot_qq(ax3)\n",
    "        \n",
    "        # 4. ACF comparison\n",
    "        ax4 = fig.add_subplot(gs[1, 2])\n",
    "        self._plot_acf(ax4)\n",
    "        \n",
    "        # 5. Spectral analysis\n",
    "        ax5 = fig.add_subplot(gs[2, :2])\n",
    "        self._plot_spectral(ax5)\n",
    "        \n",
    "        # 6. Return distribution\n",
    "        ax6 = fig.add_subplot(gs[2, 2])\n",
    "        self._plot_returns(ax6)\n",
    "        \n",
    "        # 7. Extreme value analysis\n",
    "        ax7 = fig.add_subplot(gs[3, :])\n",
    "        self._plot_extremes(ax7)\n",
    "        \n",
    "        # 8. Rolling statistics\n",
    "        ax8 = fig.add_subplot(gs[4, :])\n",
    "        self._plot_rolling_stats(ax8)\n",
    "        \n",
    "        # 9. Metric summary\n",
    "        ax9 = fig.add_subplot(gs[5:7, :])\n",
    "        self._plot_metric_summary(ax9)\n",
    "        \n",
    "        # 10. Bootstrap results\n",
    "        if self.bootstrap_results:\n",
    "            ax10 = fig.add_subplot(gs[7, :])\n",
    "            self._plot_bootstrap_results(ax10)\n",
    "        \n",
    "        plt.suptitle('Comprehensive Synthetic vs Real Data Evaluation', fontsize=16, y=0.995)\n",
    "        return fig\n",
    "    \n",
    "    def _plot_time_series(self, ax):\n",
    "        \"\"\"Plot time series comparison\"\"\"\n",
    "        time = np.arange(len(self.y_real))\n",
    "        ax.plot(time, self.y_real, 'b-', alpha=0.7, linewidth=0.5, label='Real')\n",
    "        ax.plot(time, self.y_synth, 'r-', alpha=0.7, linewidth=0.5, label='Synthetic')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.set_title('Time Series Comparison')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "    def _plot_distributions(self, ax):\n",
    "        \"\"\"Plot distribution comparison\"\"\"\n",
    "        ax.hist(self.y_real, bins=50, alpha=0.5, density=True, label='Real', color='blue')\n",
    "        ax.hist(self.y_synth, bins=50, alpha=0.5, density=True, label='Synthetic', color='red')\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title('Distribution Comparison')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add KS statistic\n",
    "        ks_stat = self.results.get('ks_statistic', np.nan)\n",
    "        ax.text(0.05, 0.95, f'KS stat: {ks_stat:.3f}', transform=ax.transAxes, \n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "    def _plot_qq(self, ax):\n",
    "        \"\"\"Q-Q plot\"\"\"\n",
    "        stats.probplot(self.y_real, dist=\"norm\", plot=ax)\n",
    "        ax.get_lines()[0].set_color('blue')\n",
    "        ax.get_lines()[0].set_label('Real')\n",
    "        \n",
    "        # Clear and replot for synthetic\n",
    "        stats.probplot(self.y_synth, dist=\"norm\", plot=ax)\n",
    "        ax.get_lines()[2].set_color('red')\n",
    "        ax.get_lines()[2].set_label('Synthetic')\n",
    "        \n",
    "        ax.set_title('Q-Q Plot Comparison')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "    def _plot_acf(self, ax):\n",
    "        \"\"\"Plot ACF comparison\"\"\"\n",
    "        lags = range(31)\n",
    "        acf_real = acf(self.y_real, nlags=30, fft=True)\n",
    "        acf_synth = acf(self.y_synth, nlags=30, fft=True)\n",
    "        \n",
    "        ax.plot(lags, acf_real, 'b-', marker='o', markersize=4, label='Real')\n",
    "        ax.plot(lags, acf_synth, 'r-', marker='s', markersize=4, label='Synthetic')\n",
    "        ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "        ax.set_xlabel('Lag')\n",
    "        ax.set_ylabel('ACF')\n",
    "        ax.set_title('Autocorrelation Function Comparison')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "    def _plot_spectral(self, ax):\n",
    "        \"\"\"Plot spectral analysis\"\"\"\n",
    "        freq_real, psd_real = periodogram(self.y_real)\n",
    "        freq_synth, psd_synth = periodogram(self.y_synth)\n",
    "        \n",
    "        ax.semilogy(freq_real, psd_real, 'b-', alpha=0.7, label='Real')\n",
    "        ax.semilogy(freq_synth, psd_synth, 'r-', alpha=0.7, label='Synthetic')\n",
    "        ax.set_xlabel('Frequency')\n",
    "        ax.set_ylabel('Power Spectral Density')\n",
    "        ax.set_title('Spectral Analysis')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add spectral KL\n",
    "        spectral_kl = self.results.get('spectral_kl', np.nan)\n",
    "        ax.text(0.95, 0.95, f'Spectral KL: {spectral_kl:.3f}', transform=ax.transAxes, \n",
    "                horizontalalignment='right', verticalalignment='top', \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "    def _plot_returns(self, ax):\n",
    "        \"\"\"Plot return distribution\"\"\"\n",
    "        returns_real = np.diff(self.y_real)\n",
    "        returns_synth = np.diff(self.y_synth)\n",
    "        \n",
    "        ax.hist(returns_real, bins=50, alpha=0.5, density=True, label='Real', color='blue')\n",
    "        ax.hist(returns_synth, bins=50, alpha=0.5, density=True, label='Synthetic', color='red')\n",
    "        ax.set_xlabel('Returns')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title('Return Distribution')\n",
    "        ax.legend()\n",
    "        \n",
    "    def _plot_extremes(self, ax):\n",
    "        \"\"\"Plot extreme value analysis\"\"\"\n",
    "        time = np.arange(len(self.y_real))\n",
    "        \n",
    "        # Define threshold\n",
    "        threshold_real = np.percentile(np.abs(self.y_real - np.mean(self.y_real)), 95)\n",
    "        threshold_synth = np.percentile(np.abs(self.y_synth - np.mean(self.y_synth)), 95)\n",
    "        \n",
    "        # Plot with extremes highlighted\n",
    "        ax.plot(time, self.y_real, 'b-', alpha=0.3, linewidth=0.5, label='Real')\n",
    "        ax.plot(time, self.y_synth, 'r-', alpha=0.3, linewidth=0.5, label='Synthetic')\n",
    "        \n",
    "        # Highlight extremes\n",
    "        extreme_real = np.abs(self.y_real - np.mean(self.y_real)) > threshold_real\n",
    "        extreme_synth = np.abs(self.y_synth - np.mean(self.y_synth)) > threshold_synth\n",
    "        \n",
    "        ax.scatter(time[extreme_real], self.y_real[extreme_real], c='blue', s=20, alpha=0.8, label='Real extremes')\n",
    "        ax.scatter(time[extreme_synth], self.y_synth[extreme_synth], c='red', s=20, alpha=0.8, label='Synthetic extremes')\n",
    "        \n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.set_title('Extreme Value Analysis')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "    def _plot_rolling_stats(self, ax):\n",
    "        \"\"\"Plot rolling statistics\"\"\"\n",
    "        window = 50\n",
    "        \n",
    "        # Calculate rolling statistics\n",
    "        real_series = pd.Series(self.y_real)\n",
    "        synth_series = pd.Series(self.y_synth)\n",
    "        \n",
    "        roll_mean_real = real_series.rolling(window).mean()\n",
    "        roll_std_real = real_series.rolling(window).std()\n",
    "        roll_mean_synth = synth_series.rolling(window).mean()\n",
    "        roll_std_synth = synth_series.rolling(window).std()\n",
    "        \n",
    "        time = np.arange(len(self.y_real))\n",
    "        \n",
    "        # Plot mean ± std\n",
    "        ax.plot(time, roll_mean_real, 'b-', label='Real mean')\n",
    "        ax.fill_between(time, roll_mean_real - roll_std_real, roll_mean_real + roll_std_real, \n",
    "                        alpha=0.2, color='blue')\n",
    "        \n",
    "        ax.plot(time, roll_mean_synth, 'r-', label='Synthetic mean')\n",
    "        ax.fill_between(time, roll_mean_synth - roll_std_synth, roll_mean_synth + roll_std_synth, \n",
    "                        alpha=0.2, color='red')\n",
    "        \n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.set_title(f'Rolling Statistics (window={window})')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "    def _plot_metric_summary(self, ax):\n",
    "        \"\"\"Plot metric summary table\"\"\"\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Prepare data for table\n",
    "        metrics_data = []\n",
    "        \n",
    "        # Distributional metrics\n",
    "        metrics_data.append(['Distributional Tests', '', ''])\n",
    "        metrics_data.append(['KS Statistic', f\"{self.results.get('ks_statistic', np.nan):.4f}\", \n",
    "                           f\"p={self.results.get('ks_pvalue', np.nan):.4f}\"])\n",
    "        metrics_data.append(['Wasserstein Distance', f\"{self.results.get('wasserstein_distance', np.nan):.4f}\", ''])\n",
    "        metrics_data.append(['Mean Difference', f\"{self.results.get('mean_diff', np.nan):.4f}\", ''])\n",
    "        metrics_data.append(['Std Difference', f\"{self.results.get('std_diff', np.nan):.4f}\", ''])\n",
    "        \n",
    "        # Temporal metrics\n",
    "        metrics_data.append(['', '', ''])\n",
    "        metrics_data.append(['Temporal Structure', '', ''])\n",
    "        metrics_data.append(['Weighted ACF L2', f\"{self.results.get('weighted_acf_l2', np.nan):.4f}\", ''])\n",
    "        metrics_data.append(['Spectral KL', f\"{self.results.get('spectral_kl', np.nan):.4f}\", ''])\n",
    "        \n",
    "        # ML metrics\n",
    "        metrics_data.append(['', '', ''])\n",
    "        metrics_data.append(['ML Discriminability', '', ''])\n",
    "        metrics_data.append(['Discriminator AUC', f\"{self.results.get('discriminator_auc', np.nan):.4f}\", \n",
    "                           '0.5 = indistinguishable'])\n",
    "        \n",
    "        # Extreme value metrics\n",
    "        metrics_data.append(['', '', ''])\n",
    "        metrics_data.append(['Extreme Values', '', ''])\n",
    "        metrics_data.append(['95% Exceed Rate Diff', f\"{self.results.get('exceed_rate_95_diff', np.nan):.4f}\", ''])\n",
    "        \n",
    "        # Non-linear dynamics\n",
    "        metrics_data.append(['', '', ''])\n",
    "        metrics_data.append(['Non-linear Dynamics', '', ''])\n",
    "        metrics_data.append(['Hurst Exponent Diff', f\"{self.results.get('hurst_diff', np.nan):.4f}\", ''])\n",
    "        \n",
    "        # Create table\n",
    "        table = ax.table(cellText=metrics_data, \n",
    "                        colLabels=['Metric', 'Value', 'Notes'],\n",
    "                        cellLoc='left',\n",
    "                        loc='center',\n",
    "                        colWidths=[0.4, 0.3, 0.3])\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1.2, 1.5)\n",
    "        \n",
    "        # Style the table\n",
    "        for i in range(len(metrics_data)):\n",
    "            for j in range(3):\n",
    "                cell = table[(i+1, j)]\n",
    "                if metrics_data[i][0] in ['Distributional Tests', 'Temporal Structure', \n",
    "                                         'ML Discriminability', 'Extreme Values', 'Non-linear Dynamics']:\n",
    "                    cell.set_text_props(weight='bold')\n",
    "                    cell.set_facecolor('#E8E8E8')\n",
    "                    \n",
    "        ax.set_title('Comprehensive Metric Summary', pad=20, fontsize=12, weight='bold')\n",
    "        \n",
    "    def _plot_bootstrap_results(self, ax):\n",
    "        \"\"\"Plot bootstrap confidence intervals\"\"\"\n",
    "        if not self.bootstrap_results:\n",
    "            return\n",
    "            \n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Prepare data\n",
    "        metrics = list(self.bootstrap_results.keys())\n",
    "        means = [self.bootstrap_results[m]['mean'] for m in metrics]\n",
    "        ci_lower = [self.bootstrap_results[m]['ci_lower'] for m in metrics]\n",
    "        ci_upper = [self.bootstrap_results[m]['ci_upper'] for m in metrics]\n",
    "        \n",
    "        # Create horizontal bar plot\n",
    "        y_pos = np.arange(len(metrics))\n",
    "        \n",
    "        # Plot confidence intervals\n",
    "        for i, (m, lower, upper) in enumerate(zip(means, ci_lower, ci_upper)):\n",
    "            ax.plot([lower, upper], [i, i], 'b-', linewidth=2)\n",
    "            ax.plot(m, i, 'ro', markersize=8)\n",
    "            \n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(metrics)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_title('Bootstrap Confidence Intervals (95%)', fontsize=12, weight='bold')\n",
    "        ax.grid(True, axis='x', alpha=0.3)\n",
    "        ax.set_xlim(min(ci_lower) * 0.9, max(ci_upper) * 1.1)\n",
    "\n",
    "# Create visualizations\n",
    "visualizer = ComprehensiveVisualizer(y_test, y_synth_test, results, bootstrap_results)\n",
    "fig = visualizer.plot_all()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477bc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Advanced Model Comparison\n",
    "class ModelComparisonFramework:\n",
    "    def __init__(self, y_train, time_train):\n",
    "        self.y_train = y_train\n",
    "        self.time_train = time_train\n",
    "        self.models = {}\n",
    "        \n",
    "    def add_model(self, name, generator_class):\n",
    "        \"\"\"Add a model to compare\"\"\"\n",
    "        self.models[name] = generator_class(self.y_train, self.time_train)\n",
    "        \n",
    "    def compare_all(self, y_test, n_samples=1000):\n",
    "        \"\"\"Compare all models\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nEvaluating {name}...\")\n",
    "            model.fit()\n",
    "            y_synth = model.generate(n_samples)\n",
    "            y_synth_test = y_synth[len(self.y_train):]\n",
    "            \n",
    "            # Quick evaluation\n",
    "            evaluator = ComprehensiveEvaluator(y_test, y_synth_test)\n",
    "            evaluator._evaluate_distributions()\n",
    "            evaluator._evaluate_temporal_structure()\n",
    "            \n",
    "            results[name] = {\n",
    "                'ks_statistic': evaluator.results['ks_statistic'],\n",
    "                'wasserstein_distance': evaluator.results['wasserstein_distance'],\n",
    "                'weighted_acf_l2': evaluator.results['weighted_acf_l2']\n",
    "            }\n",
    "            \n",
    "        return pd.DataFrame(results).T\n",
    "\n",
    "# Example: Compare with simpler baseline\n",
    "class SimpleARModel:\n",
    "    def __init__(self, y_train, time_train):\n",
    "        self.y_train = y_train\n",
    "        self.time_train = time_train\n",
    "        \n",
    "    def fit(self):\n",
    "        from statsmodels.tsa.ar_model import AutoReg\n",
    "        self.model = AutoReg(self.y_train, lags=5)\n",
    "        self.fitted = self.model.fit()\n",
    "        \n",
    "    def generate(self, n_samples):\n",
    "        # Simple generation\n",
    "        forecast = self.fitted.forecast(n_samples - len(self.y_train))\n",
    "        return np.concatenate([self.y_train, forecast])\n",
    "\n",
    "# Run comparison\n",
    "comparison = ModelComparisonFramework(y_train, time_train)\n",
    "comparison.add_model('Advanced (STL+GAM+GARCH)', AdvancedSyntheticGenerator)\n",
    "comparison.add_model('Simple AR(5)', SimpleARModel)\n",
    "\n",
    "comparison_results = comparison.compare_all(y_test)\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(comparison_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Final Report Generation\n",
    "class SyntheticDataReport:\n",
    "    def __init__(self, results, bootstrap_results, comparison_results=None):\n",
    "        self.results = results\n",
    "        self.bootstrap_results = bootstrap_results\n",
    "        self.comparison_results = comparison_results\n",
    "        \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        report = []\n",
    "        \n",
    "        report.append(\"# Synthetic Data Generation Evaluation Report\\n\")\n",
    "        report.append(f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        # Executive Summary\n",
    "        report.append(\"## Executive Summary\\n\")\n",
    "        \n",
    "        # Key metrics\n",
    "        ks_stat = self.results.get('ks_statistic', np.nan)\n",
    "        ks_pval = self.results.get('ks_pvalue', np.nan)\n",
    "        disc_auc = self.results.get('discriminator_auc', np.nan)\n",
    "        \n",
    "        if ks_pval > 0.05:\n",
    "            dist_verdict = \"PASS ✓\"\n",
    "        else:\n",
    "            dist_verdict = \"FAIL ✗\"\n",
    "            \n",
    "        if 0.45 <= disc_auc <= 0.55:\n",
    "            disc_verdict = \"EXCELLENT ✓\"\n",
    "        elif 0.4 <= disc_auc <= 0.6:\n",
    "            disc_verdict = \"GOOD ✓\"\n",
    "        else:\n",
    "            disc_verdict = \"POOR ✗\"\n",
    "            \n",
    "        report.append(f\"- **Distribution Match**: {dist_verdict} (KS p-value: {ks_pval:.3f})\\n\")\n",
    "        report.append(f\"- **ML Discriminability**: {disc_verdict} (AUC: {disc_auc:.3f})\\n\")\n",
    "        \n",
    "        # Detailed Results\n",
    "        report.append(\"\\n## Detailed Results\\n\")\n",
    "        \n",
    "        # 1. Distributional Properties\n",
    "        report.append(\"\\n### 1. Distributional Properties\\n\")\n",
    "        report.append(f\"- KS Statistic: {ks_stat:.4f} (p-value: {ks_pval:.4f})\\n\")\n",
    "        report.append(f\"- Wasserstein Distance: {self.results.get('wasserstein_distance', np.nan):.4f}\\n\")\n",
    "        report.append(f\"- Mean Difference: {self.results.get('mean_diff', np.nan):.4f}\\n\")\n",
    "        report.append(f\"- Std Difference: {self.results.get('std_diff', np.nan):.4f}\\n\")\n",
    "        \n",
    "        # 2. Temporal Structure\n",
    "        report.append(\"\\n### 2. Temporal Structure\\n\")\n",
    "        report.append(f\"- Weighted ACF L2: {self.results.get('weighted_acf_l2', np.nan):.4f}\\n\")\n",
    "        report.append(f\"- ADF test p-values - Real: {self.results.get('adf_real_pval', np.nan):.4f}, \"\n",
    "                     f\"Synthetic: {self.results.get('adf_synth_pval', np.nan):.4f}\\n\")\n",
    "        \n",
    "        # 3. Extreme Values\n",
    "        report.append(\"\\n### 3. Extreme Value Analysis\\n\")\n",
    "        report.append(f\"- 95% Exceedance Rate Diff: {self.results.get('exceed_rate_95_diff', np.nan):.4f}\\n\")\n",
    "        report.append(f\"- Extreme Clustering Diff: {self.results.get('extreme_clustering_diff', np.nan):.4f}\\n\")\n",
    "        \n",
    "        # 4. Non-linear Dynamics\n",
    "        report.append(\"\\n### 4. Non-linear Dynamics\\n\")\n",
    "        report.append(f\"- Hurst Exponent Diff: {self.results.get('hurst_diff', np.nan):.4f}\\n\")\n",
    "        report.append(f\"- Sample Entropy Diff: {self.results.get('sample_entropy_diff', np.nan):.4f}\\n\")\n",
    "        \n",
    "        # 5. Domain Constraints\n",
    "        report.append(\"\\n### 5. Domain Constraints\\n\")\n",
    "        report.append(f\"- Temperature Range - Real: [{self.results.get('min_temp_real', np.nan):.1f}, \"\n",
    "                     f\"{self.results.get('max_temp_real', np.nan):.1f}]°C\\n\")\n",
    "        report.append(f\"- Temperature Range - Synthetic: [{self.results.get('min_temp_synth', np.nan):.1f}, \"\n",
    "                     f\"{self.results.get('max_temp_synth', np.nan):.1f}]°C\\n\")\n",
    "        report.append(f\"- Violations: {self.results.get('synth_below_absolute_zero', 0)} below absolute zero, \"\n",
    "                     f\"{self.results.get('synth_above_boiling', 0)} above boiling\\n\")\n",
    "        \n",
    "        # Bootstrap Results\n",
    "        if self.bootstrap_results:\n",
    "            report.append(\"\\n### 6. Bootstrap Confidence Intervals\\n\")\n",
    "            for metric, values in self.bootstrap_results.items():\n",
    "                report.append(f\"- {metric}: {values['mean']:.4f} \"\n",
    "                            f\"(95% CI: [{values['ci_lower']:.4f}, {values['ci_upper']:.4f}])\\n\")\n",
    "        \n",
    "        # Model Comparison\n",
    "        if self.comparison_results is not None:\n",
    "            report.append(\"\\n### 7. Model Comparison\\n\")\n",
    "            report.append(self.comparison_results.to_string())\n",
    "            report.append(\"\\n\")\n",
    "        \n",
    "        # Recommendations\n",
    "        report.append(\"\\n## Recommendations\\n\")\n",
    "        \n",
    "        if ks_pval < 0.05:\n",
    "            report.append(\"- ⚠️ Distribution match is poor. Consider:\\n\")\n",
    "            report.append(\"  - Adding mixture components\\n\")\n",
    "            report.append(\"  - Using kernel density estimation\\n\")\n",
    "            report.append(\"  - Incorporating quantile matching\\n\")\n",
    "            \n",
    "        if disc_auc > 0.6:\n",
    "            report.append(\"- ⚠️ Synthetic data is too easily distinguishable. Consider:\\n\")\n",
    "            report.append(\"  - Adding more complex temporal dependencies\\n\")\n",
    "            report.append(\"  - Improving extreme value modeling\\n\")\n",
    "            report.append(\"  - Using adversarial training\\n\")\n",
    "            \n",
    "        if self.results.get('exceed_rate_95_diff', 0) > 0.01:\n",
    "            report.append(\"- ⚠️ Extreme value behavior differs significantly. Consider:\\n\")\n",
    "            report.append(\"  - Using EVT (Extreme Value Theory) models\\n\")\n",
    "            report.append(\"  - Separate modeling of tail behavior\\n\")\n",
    "            \n",
    "        return '\\n'.join(report)\n",
    "\n",
    "# Generate report\n",
    "report_generator = SyntheticDataReport(results, bootstrap_results, comparison_results)\n",
    "report = report_generator.generate_report()\n",
    "\n",
    "# Display report\n",
    "display(Markdown(report))\n",
    "\n",
    "# Save report\n",
    "with open('synthetic_data_evaluation_report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "print(\"\\nReport saved to 'synthetic_data_evaluation_report.md'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Interactive Dashboard (Optional)\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    \n",
    "    class InteractiveDashboard:\n",
    "        def __init__(self, generator, data_handler, evaluator):\n",
    "            self.generator = generator\n",
    "            self.data_handler = data_handler\n",
    "            self.evaluator = evaluator\n",
    "            \n",
    "        def create_dashboard(self):\n",
    "            \"\"\"Create interactive dashboard for parameter tuning\"\"\"\n",
    "            \n",
    "            # Controls\n",
    "            n_samples_slider = widgets.IntSlider(\n",
    "                value=1000, min=100, max=5000, step=100,\n",
    "                description='N Samples:'\n",
    "            )\n",
    "            \n",
    "            include_extremes = widgets.Checkbox(\n",
    "                value=True,\n",
    "                description='Include Extremes'\n",
    "            )\n",
    "            \n",
    "            metric_dropdown = widgets.Dropdown(\n",
    "                options=['KS Statistic', 'Wasserstein Distance', 'ACF L2'],\n",
    "                value='KS Statistic',\n",
    "                description='Metric:'\n",
    "            )\n",
    "            \n",
    "            generate_button = widgets.Button(\n",
    "                description='Generate & Evaluate',\n",
    "                button_style='primary'\n",
    "            )\n",
    "            \n",
    "            output = widgets.Output()\n",
    "            \n",
    "            def on_generate_click(b):\n",
    "                with output:\n",
    "                    output.clear_output()\n",
    "                    \n",
    "                    # Generate\n",
    "                    y_synth = self.generator.generate(\n",
    "                        n_samples_slider.value, \n",
    "                        include_extremes=include_extremes.value\n",
    "                    )\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    y_synth_test = y_synth[self.data_handler.train_end:]\n",
    "                    quick_eval = ComprehensiveEvaluator(\n",
    "                        self.data_handler.y_test, \n",
    "                        y_synth_test\n",
    "                    )\n",
    "                    quick_eval._evaluate_distributions()\n",
    "                    \n",
    "                    # Display\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "                    \n",
    "                    # Time series\n",
    "                    ax1.plot(self.data_handler.y_test[:200], 'b-', alpha=0.7, label='Real')\n",
    "                    ax1.plot(y_synth_test[:200], 'r-', alpha=0.7, label='Synthetic')\n",
    "                    ax1.set_title('Time Series Sample')\n",
    "                    ax1.legend()\n",
    "                    \n",
    "                    # Distribution\n",
    "                    ax2.hist(self.data_handler.y_test, bins=50, alpha=0.5, density=True, label='Real')\n",
    "                    ax2.hist(y_synth_test, bins=50, alpha=0.5, density=True, label='Synthetic')\n",
    "                    ax2.set_title('Distribution')\n",
    "                    ax2.legend()\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Show metric\n",
    "                    if metric_dropdown.value == 'KS Statistic':\n",
    "                        value = quick_eval.results['ks_statistic']\n",
    "                    elif metric_dropdown.value == 'Wasserstein Distance':\n",
    "                        value = quick_eval.results['wasserstein_distance']\n",
    "                    else:\n",
    "                        value = quick_eval.results['weighted_acf_l2']\n",
    "                        \n",
    "                    print(f\"\\n{metric_dropdown.value}: {value:.4f}\")\n",
    "            \n",
    "            generate_button.on_click(on_generate_click)\n",
    "            \n",
    "            # Layout\n",
    "            controls = widgets.VBox([\n",
    "                widgets.HTML(\"<h3>Synthetic Data Generator Dashboard</h3>\"),\n",
    "                n_samples_slider,\n",
    "                include_extremes,\n",
    "                metric_dropdown,\n",
    "                generate_button\n",
    "            ])\n",
    "            \n",
    "            return widgets.VBox([controls, output])\n",
    "    \n",
    "    # Create dashboard\n",
    "    dashboard = InteractiveDashboard(generator, data_handler, evaluator)\n",
    "    display(dashboard.create_dashboard())\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"ipywidgets not available. Skipping interactive dashboard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de4fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save and Load Models\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "class ModelPersistence:\n",
    "    @staticmethod\n",
    "    def save_generator(generator, filepath):\n",
    "        \"\"\"Save generator to file\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(generator, f)\n",
    "        print(f\"Generator saved to {filepath}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_generator(filepath):\n",
    "        \"\"\"Load generator from file\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            generator = pickle.load(f)\n",
    "        print(f\"Generator loaded from {filepath}\")\n",
    "        return generator\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_results(results, bootstrap_results, filepath):\n",
    "        \"\"\"Save evaluation results\"\"\"\n",
    "        save_dict = {\n",
    "            'results': results,\n",
    "            'bootstrap_results': bootstrap_results,\n",
    "            'timestamp': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Convert numpy values to native Python types\n",
    "        def convert_types(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_types(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_types(v) for v in obj]\n",
    "            return obj\n",
    "        \n",
    "        save_dict = convert_types(save_dict)\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(save_dict, f, indent=2)\n",
    "        print(f\"Results saved to {filepath}\")\n",
    "\n",
    "# Save models and results\n",
    "ModelPersistence.save_generator(generator, 'synthetic_generator.pkl')\n",
    "ModelPersistence.save_results(results, bootstrap_results, 'evaluation_results.json')\n",
    "\n",
    "print(\"\\n✅ Comprehensive evaluation complete!\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- synthetic_generator.pkl\")\n",
    "print(\"- evaluation_results.json\")\n",
    "print(\"- synthetic_data_evaluation_report.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aaai26-WCh-D8Il",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
